{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFCBbQ6WUKI5"
   },
   "source": [
    "# 加载谷歌云"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yF8x9g3_Nf3k"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wYkorvYdle8m"
   },
   "outputs": [],
   "source": [
    "! kaggle datasets download -d mateuszbuda/lgg-mri-segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E4Y4Y6kunDKp"
   },
   "outputs": [],
   "source": [
    "mkdir /root/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MiKUeNtCzaRJ"
   },
   "outputs": [],
   "source": [
    "mv /content/kaggle.json /root/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K47NuvXPzmRp"
   },
   "outputs": [],
   "source": [
    "! unzip /content/lgg-mri-segmentation.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T10:05:43.055270Z",
     "iopub.status.busy": "2022-05-09T10:05:43.054845Z",
     "iopub.status.idle": "2022-05-09T10:05:51.679726Z",
     "shell.execute_reply": "2022-05-09T10:05:51.678189Z",
     "shell.execute_reply.started": "2022-05-09T10:05:43.055232Z"
    }
   },
   "outputs": [],
   "source": [
    "! pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T12:43:17.448569Z",
     "iopub.status.busy": "2022-05-09T12:43:17.448311Z",
     "iopub.status.idle": "2022-05-09T12:43:18.705595Z",
     "shell.execute_reply": "2022-05-09T12:43:18.704577Z",
     "shell.execute_reply.started": "2022-05-09T12:43:17.448539Z"
    }
   },
   "outputs": [],
   "source": [
    "! rm -rf *.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJR9d7QlYkit"
   },
   "source": [
    "# 加载包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T10:05:51.682502Z",
     "iopub.status.busy": "2022-05-09T10:05:51.682285Z",
     "iopub.status.idle": "2022-05-09T10:05:51.691162Z",
     "shell.execute_reply": "2022-05-09T10:05:51.689984Z",
     "shell.execute_reply.started": "2022-05-09T10:05:51.682476Z"
    },
    "id": "dnhMsZadYkiu"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 \n",
    "import matplotlib.pyplot as plt \n",
    "from glob import glob\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as ff\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8gKi0swTDS7"
   },
   "source": [
    "# 加载Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3EBLU6DYkiu"
   },
   "source": [
    "## 生成pandas格式的列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T10:05:51.692863Z",
     "iopub.status.busy": "2022-05-09T10:05:51.692449Z",
     "iopub.status.idle": "2022-05-09T10:05:51.808382Z",
     "shell.execute_reply": "2022-05-09T10:05:51.807581Z",
     "shell.execute_reply.started": "2022-05-09T10:05:51.692820Z"
    },
    "id": "t-YaYL6XxBK2",
    "outputId": "37eaf915-4bfa-4845-d5b5-e49753f73cb2"
   },
   "outputs": [],
   "source": [
    "# 数据集路径\n",
    "# data_dir = \"/content/kaggle_3m\"   \n",
    "data_dir = \"../input/lgg-mri-segmentation/kaggle_3m\"\n",
    "\n",
    "# 生成mask和img的路径地址array\n",
    "images_dir = []\n",
    "masks_dir = []\n",
    "masks_dir = glob(data_dir + '/*/*_mask*')\n",
    "\n",
    "for i in masks_dir:\n",
    "    images_dir.append(i.replace('_mask',''))\n",
    "\n",
    "print(\"image的长度{}, image前两张{}\".format(len(images_dir), images_dir[:2]))\n",
    "print(\"mask的长度{}, mask前两张{}\".format(len(masks_dir), masks_dir[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T10:05:51.809893Z",
     "iopub.status.busy": "2022-05-09T10:05:51.809656Z",
     "iopub.status.idle": "2022-05-09T10:05:51.825640Z",
     "shell.execute_reply": "2022-05-09T10:05:51.824968Z",
     "shell.execute_reply.started": "2022-05-09T10:05:51.809859Z"
    },
    "id": "8XWZPt8z0EbJ",
    "outputId": "038a4614-b135-4224-f149-e6c085378de4"
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'images':images_dir,'masks':masks_dir})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goplJYDqYkiw"
   },
   "source": [
    "## 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T10:05:51.827569Z",
     "iopub.status.busy": "2022-05-09T10:05:51.826959Z",
     "iopub.status.idle": "2022-05-09T10:05:52.997867Z",
     "shell.execute_reply": "2022-05-09T10:05:52.997069Z",
     "shell.execute_reply.started": "2022-05-09T10:05:51.827531Z"
    },
    "id": "9vPagO_B0tJ_",
    "outputId": "efb3636f-24d1-44c2-f15a-bc25e2175952"
   },
   "outputs": [],
   "source": [
    "# 可视化第\n",
    "pic_list=[265,895,95]\n",
    "for N in pic_list:\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(1,3,1)\n",
    "    img = cv2.imread(data.images.iloc[N])\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.title(\"original\")\n",
    "    plt.imshow(img)\n",
    "    plt.subplot(1,3,2)\n",
    "    msk=cv2.imread(data.masks.iloc[N])\n",
    "    msk = cv2.cvtColor(msk, cv2.COLOR_BGR2RGB)\n",
    "    plt.title(\"label\")\n",
    "    plt.imshow(msk)\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.title(\"mask\")\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(msk,alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T10:05:52.999599Z",
     "iopub.status.busy": "2022-05-09T10:05:52.999360Z",
     "iopub.status.idle": "2022-05-09T10:05:53.005289Z",
     "shell.execute_reply": "2022-05-09T10:05:53.004629Z",
     "shell.execute_reply.started": "2022-05-09T10:05:52.999567Z"
    },
    "id": "87WEBb5nYkiw"
   },
   "outputs": [],
   "source": [
    "a = cv2.imread(data.masks.iloc[95])\n",
    "a = cv2.cvtColor(a, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T10:05:53.009283Z",
     "iopub.status.busy": "2022-05-09T10:05:53.008367Z",
     "iopub.status.idle": "2022-05-09T10:05:53.014857Z",
     "shell.execute_reply": "2022-05-09T10:05:53.014090Z",
     "shell.execute_reply.started": "2022-05-09T10:05:53.009244Z"
    },
    "id": "M-C0sAYLYkix",
    "outputId": "e4698ffa-4111-476c-cf6e-561d16f64294"
   },
   "outputs": [],
   "source": [
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T10:05:53.016753Z",
     "iopub.status.busy": "2022-05-09T10:05:53.016288Z",
     "iopub.status.idle": "2022-05-09T10:05:53.028962Z",
     "shell.execute_reply": "2022-05-09T10:05:53.028055Z",
     "shell.execute_reply.started": "2022-05-09T10:05:53.016715Z"
    },
    "id": "a8OxX4211YWA",
    "outputId": "c54e60cb-e0c4-4a71-9dbb-0908dcbf7962"
   },
   "outputs": [],
   "source": [
    "# 查看值得分布\n",
    "msk=cv2.imread(data.masks.iloc[95])\n",
    "print(msk.shape)\n",
    "print(np.unique(msk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EfDKAKQFYkix"
   },
   "source": [
    "## 对label编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T10:05:53.032048Z",
     "iopub.status.busy": "2022-05-09T10:05:53.031282Z",
     "iopub.status.idle": "2022-05-09T10:05:53.041420Z",
     "shell.execute_reply": "2022-05-09T10:05:53.040567Z",
     "shell.execute_reply.started": "2022-05-09T10:05:53.032010Z"
    },
    "id": "Rei3FFd59yK5"
   },
   "outputs": [],
   "source": [
    "class LabelProcessor:\n",
    "    \"\"\"对标签图像的编码, 生成1通道的每个像素是类别的array\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.colormap = self.read_color_map()\n",
    "        self.cm2lbl = self.encode_label_pix(self.colormap)\n",
    "    \n",
    "    # 标签编码，返回 1通道 的 已编码的标签 eg: [0000000][0011000][0000000]\n",
    "    def encode_label_img(self, img):\n",
    "        data = np.array(img, dtype='int32')\n",
    "        idx = (data[:, :, 0] * 256 + data[:, :, 1]) * 256 + data[:, :, 2]\n",
    "        return np.array(self.cm2lbl[idx], dtype='int64')\n",
    "    \n",
    "    # 加载color map, eg: 第0个->黑色, 第1个->白色\n",
    "    @staticmethod\n",
    "    def read_color_map():  \n",
    "        colormap = []\n",
    "        colormap.append([0,0,0])\n",
    "        colormap.append([255,255,255])\n",
    "        return colormap\n",
    "    \n",
    "    # 标签编码，返回哈希表 eg: cm2lbl[0] = 0, cm2lbl[(255*256+255)*256+256] = 1\n",
    "    @staticmethod\n",
    "    def encode_label_pix(colormap):     \n",
    "        cm2lbl = np.zeros(256 ** 3)\n",
    "        for i, cm in enumerate(colormap):\n",
    "            cm2lbl[(cm[0] * 256 + cm[1]) * 256 + cm[2]] = i\n",
    "        return cm2lbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9coh09UYkiy"
   },
   "source": [
    "## 构建pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T10:05:53.043302Z",
     "iopub.status.busy": "2022-05-09T10:05:53.043001Z",
     "iopub.status.idle": "2022-05-09T10:05:53.055418Z",
     "shell.execute_reply": "2022-05-09T10:05:53.054617Z",
     "shell.execute_reply.started": "2022-05-09T10:05:53.043258Z"
    },
    "id": "SOUhRHHc9zBu"
   },
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "    \"\"\"传入pandas格式 eg: img_path = pandas[\"image\"]\"\"\"\n",
    "    \n",
    "    def __init__(self, img_path, label_path):\n",
    "        # 读入图片和标签路径, 传入pandas格式 eg: img_path = pandas[\"image\"]\n",
    "        if not isinstance(img_path, np.ndarray):\n",
    "            self.img_path = img_path.to_numpy()\n",
    "            self.label_path = label_path.to_numpy()\n",
    "        self.labelProcessor = LabelProcessor()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.img_path[index]\n",
    "        label = self.label_path[index]\n",
    "        # 从文件名中读取数据（图片和标签都是png格式的图像数据）\n",
    "        img = cv2.imread(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        label = cv2.imread(label)\n",
    "        label = cv2.cvtColor(label, cv2.COLOR_BGR2RGB)\n",
    "        # transform\n",
    "        img, label = self.img_transform(img, label)\n",
    "\n",
    "        return {'img': img, 'label': label}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "    def img_transform(self, img, label):\n",
    "        # 对图片和标签做一些数值处理\n",
    "        transform_img = transforms.Compose([transforms.ToTensor(),  # 转tensor\n",
    "                                            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "        \n",
    "        img = transform_img(img)\n",
    "        label = self.labelProcessor.encode_label_img(label)\n",
    "        label = torch.from_numpy(label)\n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T10:05:53.057390Z",
     "iopub.status.busy": "2022-05-09T10:05:53.056889Z",
     "iopub.status.idle": "2022-05-09T10:05:53.137166Z",
     "shell.execute_reply": "2022-05-09T10:05:53.136458Z",
     "shell.execute_reply.started": "2022-05-09T10:05:53.057354Z"
    },
    "id": "HqXHMk1GUCh4",
    "outputId": "4ddbfa80-6bcf-413f-a82a-7b0dfe8d49b5"
   },
   "outputs": [],
   "source": [
    "a = MRIDataset(data[\"images\"], data[\"masks\"])\n",
    "a[0][\"img\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcr7A0hDab4b"
   },
   "source": [
    "# UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T10:05:53.138820Z",
     "iopub.status.busy": "2022-05-09T10:05:53.138568Z",
     "iopub.status.idle": "2022-05-09T10:05:53.142838Z",
     "shell.execute_reply": "2022-05-09T10:05:53.141932Z",
     "shell.execute_reply.started": "2022-05-09T10:05:53.138778Z"
    },
    "id": "uwGsRM9XBaca"
   },
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T10:20:13.005106Z",
     "iopub.status.busy": "2022-05-09T10:20:13.004586Z",
     "iopub.status.idle": "2022-05-09T10:20:19.353514Z",
     "shell.execute_reply": "2022-05-09T10:20:19.352758Z",
     "shell.execute_reply.started": "2022-05-09T10:20:13.005070Z"
    },
    "id": "zSljndIGahFr",
    "outputId": "24e3c9ed-8488-47dd-be67-af5952f28a35"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def fixed_padding(inputs, kernel_size, dilation):\n",
    "    kernel_size_effective = kernel_size + (kernel_size - 1) * (dilation - 1)\n",
    "    pad_total = kernel_size_effective - 1\n",
    "    pad_beg = pad_total // 2\n",
    "    pad_end = pad_total - pad_beg\n",
    "    padded_inputs = F.pad(inputs, (pad_beg, pad_end, pad_beg, pad_end))\n",
    "    return padded_inputs\n",
    "\n",
    "\n",
    "class SeparableConv2d_same(nn.Module):\n",
    "    def __init__(self, inplanes, planes, kernel_size=3, stride=1, dilation=1, bias=False):\n",
    "        super(SeparableConv2d_same, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inplanes, inplanes, kernel_size, stride, 0, dilation,\n",
    "                               groups=inplanes, bias=bias)\n",
    "        self.pointwise = nn.Conv2d(inplanes, planes, 1, 1, 0, 1, 1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = fixed_padding(x, self.conv1.kernel_size[0], dilation=self.conv1.dilation[0])\n",
    "        x = self.conv1(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, inplanes, planes, reps, stride=1, dilation=1, start_with_relu=True, grow_first=True, is_last=False):\n",
    "        super(Block, self).__init__()\n",
    "\n",
    "        if planes != inplanes or stride != 1:\n",
    "            self.skip = nn.Conv2d(inplanes, planes, 1, stride=stride, bias=False)\n",
    "            self.skipbn = nn.BatchNorm2d(planes)\n",
    "        else:\n",
    "            self.skip = None\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        rep = []\n",
    "\n",
    "        filters = inplanes\n",
    "        if grow_first:\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConv2d_same(inplanes, planes, 3, stride=1, dilation=dilation))\n",
    "            rep.append(nn.BatchNorm2d(planes))\n",
    "            filters = planes\n",
    "\n",
    "        for i in range(reps - 1):\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConv2d_same(filters, filters, 3, stride=1, dilation=dilation))\n",
    "            rep.append(nn.BatchNorm2d(filters))\n",
    "\n",
    "        if not grow_first:\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConv2d_same(inplanes, planes, 3, stride=1, dilation=dilation))\n",
    "            rep.append(nn.BatchNorm2d(planes))\n",
    "\n",
    "        if not start_with_relu:\n",
    "            rep = rep[1:]\n",
    "\n",
    "        if stride != 1:\n",
    "            rep.append(SeparableConv2d_same(planes, planes, 3, stride=2))\n",
    "\n",
    "        if stride == 1 and is_last:\n",
    "            rep.append(SeparableConv2d_same(planes, planes, 3, stride=1))\n",
    "\n",
    "        self.rep = nn.Sequential(*rep)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = self.rep(inp)\n",
    "\n",
    "        if self.skip is not None:\n",
    "            skip = self.skip(inp)\n",
    "            skip = self.skipbn(skip)\n",
    "        else:\n",
    "            skip = inp\n",
    "\n",
    "        x += skip\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Xception(nn.Module):\n",
    "    \"\"\"\n",
    "    Modified Alighed Xception\n",
    "    \"\"\"\n",
    "    def __init__(self, inplanes=3, os=16):\n",
    "        super(Xception, self).__init__()\n",
    "\n",
    "        if os == 16:\n",
    "            entry_block3_stride = 2\n",
    "            middle_block_dilation = 1\n",
    "            exit_block_dilations = (1, 2)\n",
    "        elif os == 8:\n",
    "            entry_block3_stride = 1\n",
    "            middle_block_dilation = 2\n",
    "            exit_block_dilations = (2, 4)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "        # Entry flow\n",
    "        self.conv1 = nn.Conv2d(inplanes, 32, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.block1 = Block(64, 128, reps=2, stride=2, start_with_relu=False)\n",
    "        self.block2 = Block(128, 256, reps=2, stride=2, start_with_relu=True, grow_first=True)\n",
    "        self.block3 = Block(256, 728, reps=2, stride=entry_block3_stride, start_with_relu=True, grow_first=True,\n",
    "                            is_last=True)\n",
    "\n",
    "        # Middle flow\n",
    "        self.block4 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n",
    "        self.block5 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n",
    "        self.block6 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n",
    "        self.block7 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n",
    "        self.block8 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n",
    "        self.block9 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n",
    "        self.block10 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n",
    "        self.block11 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n",
    "        self.block12 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n",
    "        self.block13 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n",
    "        self.block14 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n",
    "        self.block15 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n",
    "        self.block16 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n",
    "        self.block17 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n",
    "        self.block18 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n",
    "        self.block19 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n",
    "\n",
    "        # Exit flow\n",
    "        self.block20 = Block(728, 1024, reps=2, stride=1, dilation=exit_block_dilations[0],\n",
    "                             start_with_relu=True, grow_first=False, is_last=True)\n",
    "\n",
    "        self.conv3 = SeparableConv2d_same(1024, 1536, 3, stride=1, dilation=exit_block_dilations[1])\n",
    "        self.bn3 = nn.BatchNorm2d(1536)\n",
    "\n",
    "        self.conv4 = SeparableConv2d_same(1536, 1536, 3, stride=1, dilation=exit_block_dilations[1])\n",
    "        self.bn4 = nn.BatchNorm2d(1536)\n",
    "\n",
    "        self.conv5 = SeparableConv2d_same(1536, 2048, 3, stride=1, dilation=exit_block_dilations[1])\n",
    "        self.bn5 = nn.BatchNorm2d(2048)\n",
    "\n",
    "        # Init weights\n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Entry flow\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.block1(x)\n",
    "        low_level_feat = x\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "\n",
    "        # Middle flow\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        x = self.block7(x)\n",
    "        x = self.block8(x)\n",
    "        x = self.block9(x)\n",
    "        x = self.block10(x)\n",
    "        x = self.block11(x)\n",
    "        x = self.block12(x)\n",
    "        x = self.block13(x)\n",
    "        x = self.block14(x)\n",
    "        x = self.block15(x)\n",
    "        x = self.block16(x)\n",
    "        x = self.block17(x)\n",
    "        x = self.block18(x)\n",
    "        x = self.block19(x)\n",
    "\n",
    "        # Exit flow\n",
    "        x = self.block20(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x, low_level_feat\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "class ASPP_module(nn.Module):\n",
    "    def __init__(self, inplanes, planes, os):\n",
    "        super(ASPP_module, self).__init__()\n",
    "        # ASPP\n",
    "        if os == 16:\n",
    "            dilations = [1, 6, 12, 18]\n",
    "        elif os == 8:\n",
    "            dilations = [1, 12, 24, 36]\n",
    "\n",
    "        self.aspp1 = nn.Sequential(nn.Conv2d(inplanes, planes, kernel_size=1, stride=1,\n",
    "                                                          padding=0, dilation=dilations[0], bias=False),\n",
    "                                                nn.BatchNorm2d(planes),\n",
    "                                                nn.ReLU())\n",
    "        self.aspp2 = nn.Sequential(nn.Conv2d(inplanes, planes, kernel_size=3, stride=1,\n",
    "                                                           padding=dilations[1], dilation=dilations[1], bias=False),\n",
    "                                                nn.BatchNorm2d(planes),\n",
    "                                                nn.ReLU())\n",
    "        self.aspp3 = nn.Sequential(nn.Conv2d(inplanes, planes, kernel_size=3, stride=1,\n",
    "                                                           padding=dilations[2], dilation=dilations[2], bias=False),\n",
    "                                                nn.BatchNorm2d(planes),\n",
    "                                                nn.ReLU())\n",
    "        self.aspp4 = nn.Sequential(nn.Conv2d(inplanes, planes, kernel_size=3, stride=1,\n",
    "                                                           padding=dilations[3], dilation=dilations[3], bias=False),\n",
    "                                                 nn.BatchNorm2d(planes),\n",
    "                                                 nn.ReLU())\n",
    "        self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                                             nn.Conv2d(2048, 256, 1, stride=1, bias=False),\n",
    "                                             nn.BatchNorm2d(256),\n",
    "                                             nn.ReLU())\n",
    "        self.conv1 = nn.Conv2d(1280, 256, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.aspp1(x)\n",
    "        x2 = self.aspp2(x)\n",
    "        x3 = self.aspp3(x)\n",
    "        x4 = self.aspp4(x)\n",
    "        x5 = self.global_avg_pool(x)\n",
    "        x5 = F.interpolate(x5, size=x4.size()[2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "class DeepLabv3_plus(nn.Module):\n",
    "    def __init__(self, nInputChannels=3, n_classes=21, os=16, _print=True):\n",
    "        super(DeepLabv3_plus, self).__init__()\n",
    "\n",
    "        # Atrous Conv\n",
    "        self.xception_features = Xception(nInputChannels, os)\n",
    "\n",
    "        self.ASPP = ASPP_module(2048, 256, 16)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1280, 256, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # adopt [1x1, 48] for channel reduction.\n",
    "        self.conv2 = nn.Conv2d(128, 48, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(48)\n",
    "\n",
    "        self.last_conv = nn.Sequential(nn.Conv2d(304, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                       nn.BatchNorm2d(256),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                       nn.BatchNorm2d(256),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Conv2d(256, n_classes, kernel_size=1, stride=1))\n",
    "\n",
    "    def forward(self, input):\n",
    "        x, low_level_features = self.xception_features(input)\n",
    "        x = self.ASPP(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = F.interpolate(x, size=(int(math.ceil(input.size()[-2]/4)),\n",
    "                                int(math.ceil(input.size()[-1]/4))), mode='bilinear', align_corners=True)\n",
    "\n",
    "        low_level_features = self.conv2(low_level_features)\n",
    "        low_level_features = self.bn2(low_level_features)\n",
    "        low_level_features = self.relu(low_level_features)\n",
    "\n",
    "        x = torch.cat((x, low_level_features), dim=1)\n",
    "        x = self.last_conv(x)\n",
    "        x = F.interpolate(x, size=input.size()[2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = DeepLabv3_plus(nInputChannels=3, n_classes=12, os=16, _print=False)\n",
    "    model.eval()\n",
    "    image = torch.randn(1, 3, 352, 480)\n",
    "    output = model(image)\n",
    "    print(output.size())\n",
    "    summary(model, (3, 512, 512), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRgro6DBbyNQ"
   },
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T10:20:25.106319Z",
     "iopub.status.busy": "2022-05-09T10:20:25.106063Z",
     "iopub.status.idle": "2022-05-09T10:20:25.110417Z",
     "shell.execute_reply": "2022-05-09T10:20:25.109336Z",
     "shell.execute_reply.started": "2022-05-09T10:20:25.106292Z"
    }
   },
   "outputs": [],
   "source": [
    "a,b= np.array([1,2,3,4]).reshape(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T10:20:25.396098Z",
     "iopub.status.busy": "2022-05-09T10:20:25.395417Z",
     "iopub.status.idle": "2022-05-09T10:20:25.399707Z",
     "shell.execute_reply": "2022-05-09T10:20:25.398975Z",
     "shell.execute_reply.started": "2022-05-09T10:20:25.396061Z"
    }
   },
   "outputs": [],
   "source": [
    "y,u = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T10:20:25.797091Z",
     "iopub.status.busy": "2022-05-09T10:20:25.796695Z",
     "iopub.status.idle": "2022-05-09T10:20:25.805372Z",
     "shell.execute_reply": "2022-05-09T10:20:25.804707Z",
     "shell.execute_reply.started": "2022-05-09T10:20:25.797058Z"
    }
   },
   "outputs": [],
   "source": [
    "confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T14:39:21.780688Z",
     "iopub.status.busy": "2022-05-09T14:39:21.780076Z",
     "iopub.status.idle": "2022-05-09T14:39:21.791644Z",
     "shell.execute_reply": "2022-05-09T14:39:21.790673Z",
     "shell.execute_reply.started": "2022-05-09T14:39:21.780648Z"
    },
    "id": "hDbDd0CgcWRn"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import six\n",
    "\n",
    "\n",
    "def calc_semantic_segmentation_confusion(pred_labels, gt_labels):\n",
    "    pred_labels = pred_labels.flatten()\n",
    "    gt_labels = gt_labels.flatten()\n",
    "    confusion = confusion_matrix(gt_labels, pred_labels)\n",
    "    if len(confusion)!= 2:\n",
    "        confusion =  np.array([confusion[0][0],0,0,0]).reshape(2,2)\n",
    "    return confusion\n",
    "\n",
    "\n",
    "def calc_semantic_segmentation_iou(confusion):\n",
    "    intersection = np.diag(confusion)\n",
    "    union = np.sum(confusion, axis=1) + np.sum(confusion, axis=0) - np.diag(confusion)\n",
    "    Ciou = (intersection / (np.maximum(1.0, union)+  1e-10) )\n",
    "    mIoU = np.nanmean(Ciou)\n",
    "    return mIoU\n",
    "\n",
    "def calc_semantic_segmentation_dice(confusion):\n",
    "    a, b = confusion\n",
    "    tn, fp = a\n",
    "    fn, tp = b\n",
    "    return 2*tp/(2*tp + fn + fp+  1e-10)\n",
    "\n",
    "def eval_semantic_segmentation(pred_labels, gt_labels):\n",
    "    confusion = calc_semantic_segmentation_confusion(pred_labels, gt_labels)\n",
    "    mIoU = calc_semantic_segmentation_iou(confusion) \n",
    "    pixel_accuracy = np.nanmean(np.diag(confusion) / (confusion.sum(axis=1)+1e-10))\n",
    "    class_accuracy = np.diag(confusion) / ( confusion.sum(axis=1) +  1e-10 )\n",
    "    dice = calc_semantic_segmentation_dice(confusion)\n",
    "\n",
    "    return {'miou': mIoU,\n",
    "            'pixel_accuracy': pixel_accuracy,\n",
    "            'class_accuracy': class_accuracy,\n",
    "            'dice': dice}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-AkFLuDgMA5"
   },
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T12:43:26.637703Z",
     "iopub.status.busy": "2022-05-09T12:43:26.637411Z",
     "iopub.status.idle": "2022-05-09T12:43:26.650010Z",
     "shell.execute_reply": "2022-05-09T12:43:26.649302Z",
     "shell.execute_reply.started": "2022-05-09T12:43:26.637671Z"
    },
    "id": "JxP16rSHuTHG"
   },
   "outputs": [],
   "source": [
    "# 测试训练分割\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data,test_size=0.2, shuffle=True, random_state=233)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "num_class = 2\n",
    "\n",
    "Load_train = MRIDataset(train[\"images\"], train[\"masks\"])\n",
    "Load_val= MRIDataset(test[\"images\"], test[\"masks\"])\n",
    "\n",
    "train_data = DataLoader(Load_train, batch_size=4, shuffle=True, num_workers=1)\n",
    "val_data = DataLoader(Load_val, batch_size=4, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T12:43:27.643969Z",
     "iopub.status.busy": "2022-05-09T12:43:27.643684Z",
     "iopub.status.idle": "2022-05-09T12:43:28.552803Z",
     "shell.execute_reply": "2022-05-09T12:43:28.552058Z",
     "shell.execute_reply.started": "2022-05-09T12:43:27.643934Z"
    },
    "id": "X9uyeEiuxJwY"
   },
   "outputs": [],
   "source": [
    "unet =  DeepLabv3_plus(nInputChannels=3, n_classes=2, os=16, _print=False)\n",
    "unet = unet.to(device)\n",
    "criterion = nn.NLLLoss().to(device)\n",
    "optimizer = optim.Adam(unet.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T12:43:29.052325Z",
     "iopub.status.busy": "2022-05-09T12:43:29.051473Z",
     "iopub.status.idle": "2022-05-09T12:43:29.065570Z",
     "shell.execute_reply": "2022-05-09T12:43:29.064646Z",
     "shell.execute_reply.started": "2022-05-09T12:43:29.052282Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    model = model.eval()\n",
    "    num_class = 2\n",
    "    eval_loss = 0\n",
    "    eval_acc = 0\n",
    "    eval_miou = 0\n",
    "    eval_class_acc = 0\n",
    "    eval_dice = 0\n",
    "    error = 0\n",
    "\n",
    "    for j, sample in enumerate(val_data):\n",
    "        valImg = sample['img'].to(device)\n",
    "        valLabel = sample['label'].long().to(device)\n",
    "        \n",
    "        out = model(valImg)\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        loss = criterion(out, valLabel)\n",
    "        eval_loss = loss.item() + eval_loss\n",
    "        \n",
    "        # 评估\n",
    "        pre_label = out.max(dim=1)[1].data.cpu().numpy()\n",
    "        true_label = valLabel.data.cpu().numpy()\n",
    "        eval_metrics = eval_semantic_segmentation(pre_label, true_label)\n",
    "        eval_acc = eval_metrics['pixel_accuracy'] + eval_acc\n",
    "        eval_miou = eval_metrics['miou'] + eval_miou\n",
    "\n",
    "        eval_class_acc =  eval_metrics['class_accuracy'] + eval_class_acc\n",
    "        eval_dice = eval_metrics['dice'] + eval_dice\n",
    "\n",
    "    val_str = '|val Acc|: {:.5f}\\n|val dice|: {:.5f}\\n|val Mean IoU|: {:.5f}\\n|val_class_acc|: {:}'.format(\n",
    "        eval_acc / len(val_data),\n",
    "        eval_dice / len(val_data),\n",
    "        eval_miou / len(val_data),\n",
    "        eval_class_acc / (len(val_data)-error))\n",
    "    print(val_str)\n",
    "    \n",
    "    return eval_acc / len(val_data), eval_dice / len(val_data), eval_miou / len(val_data), eval_class_acc / (len(val_data)-error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T12:43:30.267552Z",
     "iopub.status.busy": "2022-05-09T12:43:30.267300Z",
     "iopub.status.idle": "2022-05-09T14:32:01.566964Z",
     "shell.execute_reply": "2022-05-09T14:32:01.565989Z",
     "shell.execute_reply.started": "2022-05-09T12:43:30.267523Z"
    },
    "id": "7V0yh0lUxcAS",
    "outputId": "5c87aeec-6068-4849-f552-94106169234c"
   },
   "outputs": [],
   "source": [
    "net = unet.train()\n",
    "\n",
    "best = [0]\n",
    "Epoch = 40\n",
    "train_acc_epoch = []\n",
    "train_miou_epoch = []\n",
    "train_dice_epoch = []\n",
    "train_class_acc_epoch = []\n",
    "\n",
    "test_acc_epoch = []\n",
    "test_miou_epoch = []\n",
    "test_dice_epoch = []\n",
    "test_class_acc_epoch = []\n",
    "\n",
    "# 训练轮次\n",
    "for epoch in range(Epoch):\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    train_miou = 0\n",
    "    train_dice = 0\n",
    "    train_class_acc = 0\n",
    "    error = 0\n",
    "    print('Epoch is [{}/{}]'.format(epoch + 1, Epoch))\n",
    "\n",
    "    # 训练批次\n",
    "    for i, sample in enumerate(train_data):\n",
    "        # 载入数据\n",
    "        img_data = sample['img'].to(device)\n",
    "        img_label = sample['label'].to(device)\n",
    "        # 训练\n",
    "        out = net(img_data)\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        loss = criterion(out, img_label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        # 评估\n",
    "        pre_label = out.max(dim=1)[1].data.cpu().numpy()\n",
    "        true_label = img_label.data.cpu().numpy()\n",
    "        eval_metrix = eval_semantic_segmentation(pre_label, true_label)\n",
    "        train_acc += eval_metrix['pixel_accuracy']\n",
    "        train_miou += eval_metrix['miou']\n",
    "        if len(eval_metrix['class_accuracy']) < num_class:\n",
    "            eval_metrix['class_accuracy'] = 0\n",
    "            train_class_acc = train_class_acc + eval_metrix['class_accuracy']\n",
    "            error += 1\n",
    "        else:\n",
    "            train_class_acc = train_class_acc + eval_metrix['class_accuracy']\n",
    "        train_dice += eval_metrix['dice']\n",
    "        # 打印每50次\n",
    "        if i%500 ==0:\n",
    "            print('|batch[{}/{}]|batch_loss:{:.9f}|'.format(i + 1, len(train_data), loss.item()))\n",
    "\n",
    "    metric_description = '|Train Acc|: {:.5f}\\n|Train dice|: {:.5f}\\n|Train Mean IoU|: {:.5f}\\n|Train_class_acc|: {:}'.format(\n",
    "        train_acc / len(train_data),\n",
    "        train_dice / len(train_data),\n",
    "        train_miou / len(train_data),\n",
    "        train_class_acc / (len(train_data)-error))\n",
    "\n",
    "\n",
    "    print(metric_description)\n",
    "    print(\"-----------------\")\n",
    "    test_acc, test_dice, test_miou, test_class_acc = evaluate(net)\n",
    "    print(\"-----------------\")\n",
    "    \n",
    "    test_acc_epoch.append(test_acc)\n",
    "    test_miou_epoch.append(test_miou)\n",
    "    test_dice_epoch.append(test_dice)\n",
    "    test_class_acc_epoch.append(test_class_acc)\n",
    "\n",
    "    train_acc_epoch.append(train_acc / len(train_data))\n",
    "    train_miou_epoch.append(train_miou / len(train_data))\n",
    "    train_dice_epoch.append(train_dice / len(train_data))\n",
    "    train_class_acc_epoch.append(list(train_class_acc / (len(train_data)-error)))\n",
    "\n",
    "    # 储存模型\n",
    "    if max(best) <= train_miou / len(train_data):\n",
    "        best.append(train_miou / len(train_data))\n",
    "        torch.save(net.state_dict(), './{}.pth'.format(epoch))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T12:16:48.916118Z",
     "iopub.status.busy": "2022-05-09T12:16:48.915557Z",
     "iopub.status.idle": "2022-05-09T12:16:48.933499Z",
     "shell.execute_reply": "2022-05-09T12:16:48.932505Z",
     "shell.execute_reply.started": "2022-05-09T12:16:48.916063Z"
    },
    "id": "tvPBJ-QUYki3",
    "outputId": "ebb9a1c5-ec5e-47e8-d84a-af2d2a1b7c72"
   },
   "outputs": [],
   "source": [
    "len(train_class_acc_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EjMTNYGHozY"
   },
   "source": [
    "# validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T12:30:41.767963Z",
     "iopub.status.busy": "2022-05-09T12:30:41.767163Z",
     "iopub.status.idle": "2022-05-09T12:30:41.771963Z",
     "shell.execute_reply": "2022-05-09T12:30:41.771290Z",
     "shell.execute_reply.started": "2022-05-09T12:30:41.767892Z"
    },
    "id": "zT-bXv-RM2xE"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "BATCH_SIZE = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T12:30:43.192892Z",
     "iopub.status.busy": "2022-05-09T12:30:43.192632Z",
     "iopub.status.idle": "2022-05-09T12:30:43.197646Z",
     "shell.execute_reply": "2022-05-09T12:30:43.196892Z",
     "shell.execute_reply.started": "2022-05-09T12:30:43.192861Z"
    },
    "id": "V8Z-Xv22FWcO",
    "outputId": "16f9518e-9ef3-4a20-954a-79243fce3b74"
   },
   "outputs": [],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T14:39:29.784513Z",
     "iopub.status.busy": "2022-05-09T14:39:29.784253Z",
     "iopub.status.idle": "2022-05-09T14:39:30.901570Z",
     "shell.execute_reply": "2022-05-09T14:39:30.900850Z",
     "shell.execute_reply.started": "2022-05-09T14:39:29.784485Z"
    },
    "id": "SB9Ct3NK28Zw",
    "outputId": "efb410eb-f921-4f90-efa7-375c1ae2f6dd"
   },
   "outputs": [],
   "source": [
    "net = DeepLabv3_plus(nInputChannels=3, n_classes=2, os=16, _print=False)\n",
    "net.to(device)\n",
    "net.load_state_dict(torch.load('39.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T14:39:30.954247Z",
     "iopub.status.busy": "2022-05-09T14:39:30.954049Z",
     "iopub.status.idle": "2022-05-09T14:39:30.957777Z",
     "shell.execute_reply": "2022-05-09T14:39:30.957123Z",
     "shell.execute_reply.started": "2022-05-09T14:39:30.954222Z"
    },
    "id": "O3dz79G1DP5R"
   },
   "outputs": [],
   "source": [
    "train_acc = 0\n",
    "train_miou = 0\n",
    "train_class_acc = 0\n",
    "train_mpa = 0\n",
    "error = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T14:39:32.010457Z",
     "iopub.status.busy": "2022-05-09T14:39:32.009883Z",
     "iopub.status.idle": "2022-05-09T14:39:32.019552Z",
     "shell.execute_reply": "2022-05-09T14:39:32.018799Z",
     "shell.execute_reply.started": "2022-05-09T14:39:32.010416Z"
    },
    "id": "oN-VQgNZN1EO"
   },
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    net = model.eval()\n",
    "    num_class = 2\n",
    "    eval_loss = 0\n",
    "    eval_acc = 0\n",
    "    eval_miou = 0\n",
    "    eval_class_acc = 0\n",
    "    eval_dice = 0\n",
    "    error = 0\n",
    "\n",
    "    for j, sample in enumerate(val_data):\n",
    "        valImg = sample['img'].to(device)\n",
    "        valLabel = sample['label'].long().to(device)\n",
    "        \n",
    "        out = net(valImg)\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        loss = criterion(out, valLabel)\n",
    "        eval_loss = loss.item() + eval_loss\n",
    "        \n",
    "        # 评估\n",
    "        pre_label = out.max(dim=1)[1].data.cpu().numpy()\n",
    "        true_label = valLabel.data.cpu().numpy()\n",
    "        eval_metrics = eval_semantic_segmentation(pre_label, true_label)\n",
    "        eval_acc = eval_metrics['pixel_accuracy'] + eval_acc\n",
    "        eval_miou = eval_metrics['miou'] + eval_miou\n",
    "\n",
    "        eval_class_acc =  eval_metrics['class_accuracy'] + eval_class_acc\n",
    "        eval_dice = eval_metrics['dice'] + eval_dice\n",
    "\n",
    "    val_str = '|val Acc|: {:.5f}\\n|val dice|: {:.5f}\\n|val Mean IoU|: {:.5f}\\n|val_class_acc|: {:}'.format(\n",
    "        eval_acc / len(val_data),\n",
    "        eval_dice / len(val_data),\n",
    "        eval_miou / len(val_data),\n",
    "        eval_class_acc / (len(val_data)-error))\n",
    "    print(val_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T14:39:32.837766Z",
     "iopub.status.busy": "2022-05-09T14:39:32.837039Z",
     "iopub.status.idle": "2022-05-09T14:39:49.275677Z",
     "shell.execute_reply": "2022-05-09T14:39:49.274622Z",
     "shell.execute_reply.started": "2022-05-09T14:39:32.837717Z"
    },
    "id": "oIfLQXC2T3Tj",
    "outputId": "375cd35e-42c8-4bb2-bfcc-b34cebe91621"
   },
   "outputs": [],
   "source": [
    "evaluate(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NYNfh6UUAud"
   },
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T14:39:49.278543Z",
     "iopub.status.busy": "2022-05-09T14:39:49.278244Z",
     "iopub.status.idle": "2022-05-09T14:39:49.283663Z",
     "shell.execute_reply": "2022-05-09T14:39:49.282493Z",
     "shell.execute_reply.started": "2022-05-09T14:39:49.278501Z"
    },
    "id": "aMyZgjJkU4iU"
   },
   "outputs": [],
   "source": [
    "colormap = []\n",
    "colormap.append([0,0,0])\n",
    "colormap.append([255,255,255])\n",
    "cm = np.array(colormap).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T14:39:49.445636Z",
     "iopub.status.busy": "2022-05-09T14:39:49.444854Z",
     "iopub.status.idle": "2022-05-09T14:39:49.532044Z",
     "shell.execute_reply": "2022-05-09T14:39:49.531198Z",
     "shell.execute_reply.started": "2022-05-09T14:39:49.445597Z"
    },
    "id": "VLBdljGUfflG"
   },
   "outputs": [],
   "source": [
    "test_img = test[\"images\"].to_numpy()[22:32]\n",
    "test_label = test[\"masks\"].to_numpy()[22:32]\n",
    "pd_aa = pd.DataFrame({'images':test_img,'masks':test_label})\n",
    "Load_val= MRIDataset(pd_aa['images'], pd_aa['masks'])\n",
    "val_data = DataLoader(Load_val, batch_size=10, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T14:39:49.546199Z",
     "iopub.status.busy": "2022-05-09T14:39:49.545794Z",
     "iopub.status.idle": "2022-05-09T14:39:49.689095Z",
     "shell.execute_reply": "2022-05-09T14:39:49.687837Z",
     "shell.execute_reply.started": "2022-05-09T14:39:49.546164Z"
    },
    "id": "mec9neBVV7rL"
   },
   "outputs": [],
   "source": [
    "smaples = iter(val_data)\n",
    "sample = smaples.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T14:39:49.769902Z",
     "iopub.status.busy": "2022-05-09T14:39:49.769009Z",
     "iopub.status.idle": "2022-05-09T14:39:49.796760Z",
     "shell.execute_reply": "2022-05-09T14:39:49.795970Z",
     "shell.execute_reply.started": "2022-05-09T14:39:49.769860Z"
    },
    "id": "Lnz_pbrUWe8m",
    "outputId": "a98b7d1c-5c02-4512-8422-c68110ad9cdb"
   },
   "outputs": [],
   "source": [
    "np.unique(sample[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T14:39:49.973198Z",
     "iopub.status.busy": "2022-05-09T14:39:49.972981Z",
     "iopub.status.idle": "2022-05-09T14:39:49.978438Z",
     "shell.execute_reply": "2022-05-09T14:39:49.977495Z",
     "shell.execute_reply.started": "2022-05-09T14:39:49.973171Z"
    },
    "id": "qpCl9AvsWEPQ",
    "outputId": "7725e9f5-6944-4bf3-bc46-36961e935877"
   },
   "outputs": [],
   "source": [
    "sample[\"img\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T14:39:50.334525Z",
     "iopub.status.busy": "2022-05-09T14:39:50.333808Z",
     "iopub.status.idle": "2022-05-09T14:39:50.453640Z",
     "shell.execute_reply": "2022-05-09T14:39:50.452834Z",
     "shell.execute_reply.started": "2022-05-09T14:39:50.334485Z"
    },
    "id": "4ulIlLBYVzkq"
   },
   "outputs": [],
   "source": [
    "valImg = sample['img'].to(device)\n",
    "valLabel = sample['label'].long().to(device)\n",
    "out = net(valImg)\n",
    "out = F.log_softmax(out, dim=1)\n",
    "pre_label = out.max(1)[1].squeeze().cpu().data.numpy()\n",
    "pre = cm[pre_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T14:39:50.455458Z",
     "iopub.status.busy": "2022-05-09T14:39:50.455210Z",
     "iopub.status.idle": "2022-05-09T14:39:50.482083Z",
     "shell.execute_reply": "2022-05-09T14:39:50.481299Z",
     "shell.execute_reply.started": "2022-05-09T14:39:50.455424Z"
    },
    "id": "P6n4j8tXjot9",
    "outputId": "03c5de57-b85f-4f82-cfb5-c9e24db7cc48"
   },
   "outputs": [],
   "source": [
    "true_pic = cm[sample[\"label\"]]\n",
    "true_pic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T14:39:50.623567Z",
     "iopub.status.busy": "2022-05-09T14:39:50.623371Z",
     "iopub.status.idle": "2022-05-09T14:39:57.090512Z",
     "shell.execute_reply": "2022-05-09T14:39:57.089823Z",
     "shell.execute_reply.started": "2022-05-09T14:39:50.623543Z"
    },
    "id": "5HBIMZ-SaaoO",
    "outputId": "c73182f1-1f8b-4260-ab50-b3c828f04edc"
   },
   "outputs": [],
   "source": [
    "for N in range(10):\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.subplot(1,4,1)\n",
    "    img=cv2.imread(test_img[N])\n",
    "    plt.title(\"original\")\n",
    "    plt.imshow(img)\n",
    "    plt.subplot(1,4,2)\n",
    "    plt.title(\"true\")\n",
    "    plt.imshow(true_pic[N])\n",
    "    plt.subplot(1,4,3)\n",
    "    plt.title(\"predict\")\n",
    "    plt.imshow(pre[N])\n",
    "    plt.subplot(1,4,4)\n",
    "    plt.title(\"ture&predict\")\n",
    "    plt.imshow(true_pic[N])\n",
    "    plt.imshow(pre[N],alpha=0.4,cmap=\"jet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
